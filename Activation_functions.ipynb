{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Activation Functions in Deep Learning**\n",
        "___\n",
        "## Common Activation Functions in ANNs are important in neural networks because they introduce non-linearity and helps the network to learn complex patterns. Lets see some common activation functions used in ANNs\n",
        "\n",
        "\n",
        "___\n",
        "\n",
        "#### 1. **Sigmoid** Function: Outputs values between 0 and 1. It is used in binary classification tasks like deciding if an image is a cat or not.  \n",
        "____\n",
        "\n",
        "#### 2. **ReLU (Rectified Linear Unit):** A popular choice for hidden layers, it returns the input if positive and zero otherwise. It helps to solve the vanishing gradient problem.  \n",
        "___\n",
        "\n",
        "#### 3. **Tanh (Hyperbolic Tangent):** Similar to sigmoid but outputs values between -1 and 1. It is used in hidden layers when a broader range of outputs is needed.  \n",
        "___\n",
        "#### 4. **Softmax:** Converts raw outputs into probabilities used in the final layer of a network for multi-class classification tasks."
      ],
      "metadata": {
        "id": "qMUZ92i5Q2mG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numpy for calculations\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sigmoid Function (The output from 0 to 1)\n",
        "def sigmoid(x):\n",
        "  sigma = 1 / 1 + np.exp(-x)\n",
        "  return sigma\n",
        "\n",
        "\n",
        "# ReLu f(x) = (0 , max)\n",
        "def ReLu(x):\n",
        "  relu = np.maximum(0, x)\n",
        "  return relu\n",
        "\n",
        "\n",
        "# Tanh the values between (-1 to +1)\n",
        "def Tanh(x):\n",
        "  tanh = np.sinh(x) / np.cosh(x)\n",
        "  return tanh\n",
        "\n",
        "\n",
        "# Softmax\n",
        "def Softmax(z):\n",
        "  softmax = np.exp(z) / np.exp(sum(z))\n",
        "  return softmax\n",
        "\n",
        "\n",
        "\n",
        "# Driver Activation function (All in one)\n",
        "def Activation_functions(x):\n",
        "  print(\"Sigmoid Activation--> \", sigmoid(x))\n",
        "  print(\"ReLu Activation--> \", ReLu(x))\n",
        "  print(\"Tanh Activation--> \", Tanh(x))\n",
        "  print(\"Softmax Activation--> \", Softmax([1.2, 2.3, 3.2]))\n",
        "\n",
        "\n",
        "\n",
        "Activation_functions(0.45)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhjgrNEyQx_i",
        "outputId": "c6d564a8-fe8b-4b98-e99e-6ad6c0aad0ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Activation-->  1.6376281516217732\n",
            "ReLu Activation-->  0.45\n",
            "Tanh Activation-->  0.42189900525000795\n",
            "Softmax Activation-->  [0.00408677 0.01227734 0.03019738]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}